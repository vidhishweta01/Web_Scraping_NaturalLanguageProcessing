{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('cik_list2.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df.select('SECFNAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1=k.rdd.map(lambda p:p.SECFNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt(x):\n",
    "    req=urllib.request.urlopen(\"https://www.sec.gov/Archives/\"+x)\n",
    "    st=req.read()\n",
    "    file_name=x[-13:]\n",
    "    f1=open(file_name,'a')\n",
    "    f1.write(str(st))\n",
    "    f1.close()\n",
    "    return(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2=k1.map(get_txt).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text(x):\n",
    "    kk1=x.split(\"\\\\n\")\n",
    "    return(kk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem(x):\n",
    "    if x!=[]:\n",
    "        return(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(x):\n",
    "    k=[\"\\\\t\",\"\\\\\",\"{\",\"}\",\";\",\"<\",\">\",\"(\",\")\",\"[\",\"]\",\"_\",\"/\",\"\\\\\",\":\",\"-\",\"=\",\",\",\".\",\"'\",\"%\",\"#\",\"@\",\"!\",\"^\",\"?\",\"`\",\"~\",\"`\",\"$\",\"*\",\"&\",\"''\",\"``\"]\n",
    "    for i in k:\n",
    "        x=x.replace(i,\" \")\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_num(x):\n",
    "    for i in range(0,10):\n",
    "        x=x.replace(str(i),\"\")\n",
    "    return(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_wrds(x):\n",
    "    stop_words=set(stopwords.words('english'));\n",
    "    k2=['a','b','c','d','e','i','j','k','l','``',\"''\",'m','n','o','p','q','r','s','also','t','u','v','w','z','ii','iii','iv','vi','vii','viii','ix','xi','xii','xiii','xiv','xvi','xvii','xviii','xix','xx','xxi']\n",
    "    for i in k2:\n",
    "        stop_words.add(i)\n",
    "    word_tokens=word_tokenize(x);\n",
    "    f_s=[];\n",
    "    for w in word_tokens:\n",
    "        if(w.lower() not in stop_words):\n",
    "            f_s.append(w.lower());\n",
    "    return(f_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem(x):\n",
    "    if x!=[]:\n",
    "        return(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg=spark.read.text(\"nega.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg1=neg.rdd.map(lambda p:p.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "negi=neg1.map(tokens).map(rem_num).map(stop_wrds).filter(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=negi.take(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=[]\n",
    "for i in l1:\n",
    "    for j in i:\n",
    "        l1=l1+[j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_st(x):\n",
    "    h_set=set(l1)\n",
    "    h_set.add(\"uc\")\n",
    "    f_s=[]\n",
    "    for w in x:\n",
    "        if(w not in h_set):\n",
    "            f_s.append(w);\n",
    "    return(f_s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg=negi.map(rem_st).filter(rem).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=['actionable,agreeance,end,day,back,envelope,bandwidth,bring,game,client-centered,come-to-Jesus,core,competency,CYA,drill,ducks,row,forward,initiative,going,forward,go,rogue,guesstimate,harvesting,efficiencies,hit,ground,impact,incent,incentivize,impactful,kick,down,offline,level,leverage,liaise,mission-critical,monetize,net-net,same,page,operationalize,optimize,out,pocket,paradigm,parameters,per,planful,push,envelope,pursuant,putting,lipstick,pig,recontextualize,repurpose,rightsized,sacred,cow,scalable,seamless,integration,seismic,shift,smartsized,strategic,alliance,strategic,dynamism,synergize,synergy,outside,against,wall,see,if,sticks,throw,bus,turnkey,under,radar,utilization,utilize,value-added,verbiage,win-win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "l33=[]\n",
    "for i in neg:\n",
    "    for j in i:\n",
    "        l33=l33+[j]\n",
    "l33=l33+k\n",
    "l33_set=set(l33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negi_score(x):\n",
    "    for w in x:\n",
    "        if(w in l33_set):\n",
    "            return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=spark.read.text(\"pos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=pos.rdd.map(lambda p:p.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi=positive.map(tokens).map(rem_num).map(stop_wrds).filter(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=posi.map(rem_st).filter(rem).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncer=spark.read.csv(\"uncertainty_dictionary.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc=uncer.rdd.map(lambda p:p.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc1=unc.map(lambda x:x.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unc=[]\n",
    "for i in unc1:\n",
    "    for j in i:\n",
    "        unc=unc+[j]\n",
    "unc_set=set(unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unc_score(x):\n",
    "    for w in x:\n",
    "        if(w in unc_set):\n",
    "            return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "constr=spark.read.csv(\"constraining_dictionary.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons=constr.rdd.map(lambda p:p.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons=cons.map(lambda x:x.lower()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=[]\n",
    "for i in cons:\n",
    "    for j in i:\n",
    "        con=con+[j]\n",
    "con_set=set(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_score(x):\n",
    "    for w in x:\n",
    "        if(w in con_set):\n",
    "            return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "l44=[]\n",
    "for i in pos:\n",
    "    for j in i:\n",
    "        l44=l44+[j]\n",
    "l44_set=set(l44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posi_score(x):\n",
    "    for w in x:\n",
    "        if(w in l44_set):\n",
    "            return(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex(x):\n",
    "    w=0\n",
    "    vow=('a','e','i','o','u')\n",
    "    for i in x:\n",
    "        c=0\n",
    "        for j in i:\n",
    "            l=0\n",
    "            if(j in vow and l!=(len(i)-2)):\n",
    "                c=c+1\n",
    "                l=l+1\n",
    "            if(c>2):\n",
    "                w=w+1\n",
    "    return(w)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex(x):\n",
    "    w=0\n",
    "    vow=('a','e','i','o','u')\n",
    "    for i in x:\n",
    "        c=0\n",
    "        for j in i:\n",
    "            l=0\n",
    "            if(j in vow and l!=(len(i)-2)):\n",
    "                c=c+1\n",
    "                l=l+1\n",
    "            if(c>2):\n",
    "                w=w+1\n",
    "    return(w)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[]\n",
    "n=[]\n",
    "p=[]\n",
    "pol=[]\n",
    "n_pr=[]\n",
    "p_pr=[]\n",
    "com_cnt=[]\n",
    "com_pr=[]\n",
    "fog=[]\n",
    "const=[]\n",
    "uncert=[]\n",
    "con_pro=[]\n",
    "unc_pro=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in k2[0:152]:\n",
    "    text1=spark.read.text(i)\n",
    "    kf=text1.rdd.map(lambda p:p.value)\n",
    "    kf1=kf.flatMap(get_text).map(tokens).map(rem_num).map(stop_wrds)\n",
    "    complex1=kf1.map(complex).reduce(add)\n",
    "    sen=kf1.count()\n",
    "    word_cnt=kf1.map(lambda x:len(x)).reduce(add)\n",
    "    avg_sen_len=word_cnt/sen\n",
    "    po=kf1.filter(posi_score).map(lambda x:len(x)).reduce(add)\n",
    "    ne=kf1.filter(negi_score).map(lambda x:len(x)).reduce(add)\n",
    "    con_s=kf1.filter(con_score)\n",
    "    if(con_s.collect()!=[]):\n",
    "        con_s1=con_s.map(lambda x:len(x)).reduce(add)\n",
    "        conp=(con_s1/word_cnt)*100\n",
    "    else:\n",
    "        conp=0\n",
    "    uncer_t=kf1.filter(unc_score).map(lambda x:len(x)).reduce(add)\n",
    "    \n",
    "    uncp=(uncer_t/word_cnt)*100\n",
    "    pl=(po-ne)/(po+ne)\n",
    "    p_prp=(po/word_cnt)*100\n",
    "    n_prop=(ne/word_cnt)*100\n",
    "    com_percent=(complex1/word_cnt)*100\n",
    "    fg=0.4*(avg_sen_len+com_percent)\n",
    "    m=m+[avg_sen_len]\n",
    "    n=n+[ne]\n",
    "    p=p+[po]\n",
    "    pol=pol+[pl]\n",
    "    n_pr=n_pr+[n_prop]\n",
    "    p_pr=p_pr+[p_prp]\n",
    "    com_cnt=com_cnt+[complex1]\n",
    "    com_pr=com_pr+[com_percent]\n",
    "    fog=fog+[fg]\n",
    "    uncert=uncert+[uncer_t]\n",
    "    con_pro=con_pro+[conp]\n",
    "    unc_pro=unc_pro+[uncer_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK=df.select(\"CIK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik=CIK.rdd.map(lambda p:p.CIK).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"result.txt\",\"a\")\n",
    "f.write(\"CIK,mda_positive_score,mda_negative_score,mda_polarity_score,mda_average_sentence_length,mda_percentage_of_complex_words,mda_fog_index,mda_complex_word_count,mda_uncertainty_score,mda_positive_word_proportion,mda_negative_word_proportion,mda_uncertainty_word_proportion,mda_constraining_word_proportion\\n\")\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open(\"result.txt\",\"a\")\n",
    "stri=\"\"\n",
    "for i in range(0,152):\n",
    "     stri=stri+cik[i]+\",\"+str(p[i])+\",\"+str(n[i])+\",\"+str(pol[i])+\",\"+str(m[i])+\",\"+str(com_pr[i])+\",\"+str(fog[i])+\",\"+str(com_cnt[i])+\",\"+str(uncert[i])+\",\"+str(p_pr[i])+\",\"+str(n_pr[i])+\",\"+str(unc_pro[i])+\",\"+str(con_pro[i])+\"\\n\"\n",
    "f1.write(stri)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.read.csv('result.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=df1.join(df, df1.CIK == df.CIK).select(df1.CIK,df1.mda_positive_score,df1.mda_negative_score,df1.mda_polarity_score,df1.mda_average_sentence_length,df1.mda_percentage_of_complex_words,df1.mda_fog_index,df1.mda_complex_word_count,df1.mda_uncertainty_score,df1.mda_positive_word_proportion,df1.mda_negative_word_proportion,df1.mda_uncertainty_word_proportion,df1.mda_constraining_word_proportion,df.CONAME,df.FYRMO,df.FDATE,df.FORM,df.SECFNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CIK: string (nullable = true)\n",
      " |-- mda_positive_score: string (nullable = true)\n",
      " |-- mda_negative_score: string (nullable = true)\n",
      " |-- mda_polarity_score: string (nullable = true)\n",
      " |-- mda_average_sentence_length: string (nullable = true)\n",
      " |-- mda_percentage_of_complex_words: string (nullable = true)\n",
      " |-- mda_fog_index: string (nullable = true)\n",
      " |-- mda_complex_word_count: string (nullable = true)\n",
      " |-- mda_uncertainty_score: string (nullable = true)\n",
      " |-- mda_positive_word_proportion: string (nullable = true)\n",
      " |-- mda_negative_word_proportion: string (nullable = true)\n",
      " |-- mda_uncertainty_word_proportion: string (nullable = true)\n",
      " |-- mda_constraining_word_proportion: string (nullable = true)\n",
      " |-- CONAME: string (nullable = true)\n",
      " |-- FYRMO: string (nullable = true)\n",
      " |-- FDATE: string (nullable = true)\n",
      " |-- FORM: string (nullable = true)\n",
      " |-- SECFNAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.write.csv(\"out\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
